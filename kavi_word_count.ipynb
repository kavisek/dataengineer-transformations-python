{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f1ff377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/root/.cache/pypoetry/virtualenvs/data-transformations-NkKf-v7k-py3.9/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/04/26 16:20:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/26 16:20:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from data_transformations.wordcount import word_count_transformer\n",
    "\n",
    "LOG_FILENAME = 'project.log'\n",
    "APP_NAME = \"WordCount\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(filename=LOG_FILENAME, level=logging.INFO)\n",
    "    logging.info(sys.argv)\n",
    "\n",
    "    # if len(sys.argv) is not 3:\n",
    "    #     logging.warning(\"Input .txt file and output path are required\")\n",
    "    #     sys.exit(1)\n",
    "\n",
    "    spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    app_name = sc.appName\n",
    "    logging.info(\"Application Initialized: \" + app_name)\n",
    "    input_path = \"./resources/word_count/words.txt\"\n",
    "    output_path = \"./output\"\n",
    "#     word_count_transformer.run(spark, input_path, output_path)\n",
    "#     logging.info(\"Application Done: \" + spark.sparkContext.appName)\n",
    "#     spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c91047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df = spark.read.text(input_path)\n",
    "input_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99bb1dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "|Translated by Dav...|\n",
      "|                    |\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "|almost no restric...|\n",
      "|re-use it under t...|\n",
      "|with this eBook o...|\n",
      "|                    |\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "725cd00b-9b51-4139-abb1-3fe7b4ecd0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "786"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342a45f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80e40b89-661a-49ba-adf2-3c7023d8c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               value|               words|\n",
      "+--------------------+--------------------+\n",
      "|The Project Guten...|[the, project, gu...|\n",
      "|Translated by Dav...|[translated, by, ...|\n",
      "|                    |                  []|\n",
      "|                    |                  []|\n",
      "|This eBook is for...|[this, ebook, is,...|\n",
      "|almost no restric...|[almost, no, rest...|\n",
      "|re-use it under t...|[re-use, it, unde...|\n",
      "|with this eBook o...|[with, this, eboo...|\n",
      "|                    |                  []|\n",
      "|                    |                  []|\n",
      "|** This is a COPY...|[**, this, is, a,...|\n",
      "|**     Please fol...|[**, , , , , plea...|\n",
      "|                    |                  []|\n",
      "|                    |                  []|\n",
      "|                    |                  []|\n",
      "|                    |                  []|\n",
      "|Title: Metamorphosis|[title:, metamorp...|\n",
      "|                    |                  []|\n",
      "|                    |                  []|\n",
      "| Author: Franz Kafka|[author:, franz, ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(input_df)\n",
    "tokenized.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13dfa542-9f9f-4d90-b5db-b240fe0bd044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|          |  579|\n",
      "|    \"ah!\",|    1|\n",
      "| \"alright,|    1|\n",
      "|       \"am|    1|\n",
      "|      \"and|    2|\n",
      "|    \"anna!|    1|\n",
      "|   \"aren't|    1|\n",
      "|       \"at|    1|\n",
      "|   \"before|    1|\n",
      "|      \"but|    2|\n",
      "|      \"can|    1|\n",
      "|  \"cheerio|    1|\n",
      "|    \"close|    1|\n",
      "|     \"come|    4|\n",
      "|  \"dead?\",|    1|\n",
      "|\"defects,\"|    1|\n",
      "|      \"did|    1|\n",
      "|  \"father,|    1|\n",
      "|      \"get|    1|\n",
      "|  \"getting|    1|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, explode\n",
    "tokenized.withColumn(\"word\", explode(col(\"words\"))) \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .agg(count(\"*\").alias('count')) \\\n",
    "    .sort('word', ascending=True) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d72a293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|            |  579|\n",
      "|      \"Ah!\",|    1|\n",
      "|   \"Alright,|    1|\n",
      "|         \"Am|    1|\n",
      "|        \"And|    1|\n",
      "|      \"Anna!|    1|\n",
      "|     \"Aren't|    1|\n",
      "|         \"At|    1|\n",
      "|     \"Before|    1|\n",
      "|        \"But|    1|\n",
      "|        \"Can|    1|\n",
      "|    \"Cheerio|    1|\n",
      "|      \"Close|    1|\n",
      "|       \"Come|    3|\n",
      "|    \"Dead?\",|    1|\n",
      "|  \"Defects,\"|    1|\n",
      "|        \"Did|    1|\n",
      "|    \"Father,|    1|\n",
      "|    \"Getting|    1|\n",
      "|        \"God|    1|\n",
      "|       \"Good|    1|\n",
      "|    \"Gregor!|    1|\n",
      "|   \"Gregor!\"|    1|\n",
      "|   \"Gregor\",|    2|\n",
      "|    \"Gregor,|    3|\n",
      "|    \"Gregor?|    1|\n",
      "|     \"Grete,|    1|\n",
      "|         \"He|    2|\n",
      "|       \"He's|    1|\n",
      "|      \"Help,|    1|\n",
      "|        \"How|    1|\n",
      "|          \"I|    4|\n",
      "|        \"I'd|    1|\n",
      "|       \"I'll|    3|\n",
      "|        \"I'm|    1|\n",
      "|         \"If|    2|\n",
      "|\"Information|    1|\n",
      "|         \"Is|    1|\n",
      "|       \"It's|    1|\n",
      "|       \"Just|    2|\n",
      "|      \"Leave|    1|\n",
      "|        \"Let|    1|\n",
      "|   \"Listen\",|    1|\n",
      "|      \"Maybe|    1|\n",
      "|   \"Mother's|    1|\n",
      "|    \"Mother,|    1|\n",
      "|   \"Mother?\"|    1|\n",
      "|        \"Mr.|    2|\n",
      "|         \"My|    1|\n",
      "|       \"No\",|    2|\n",
      "+------------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "x = input_df.withColumn('word', explode(split(col('value'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('word', ascending=True)\\\n",
    "\n",
    "x.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5beb841f-9f35-43b2-898a-8d1f28730f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===================================================>  (190 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     word|count|\n",
      "+---------+-----+\n",
      "|         |  579|\n",
      "|   \"Ah!\",|    1|\n",
      "|\"Alright,|    1|\n",
      "|      \"Am|    1|\n",
      "|     \"And|    1|\n",
      "|   \"Anna!|    1|\n",
      "|  \"Aren't|    1|\n",
      "|      \"At|    1|\n",
      "|  \"Before|    1|\n",
      "|     \"But|    1|\n",
      "+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "x.coalesce(1).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bbf1bc-3014-410b-b53e-33db98caa38d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
